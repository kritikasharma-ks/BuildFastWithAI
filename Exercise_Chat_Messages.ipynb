{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kritikasharma-ks/BuildFastWithAI/blob/main/Exercise_Chat_Messages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 :  Chat Messages\n",
        "Use HumanMessage and AIMessage from LangChain to interact with a Large Language Model (LLM). Ask the LLM a question about Generative AI and print its response.\n"
      ],
      "metadata": {
        "id": "LwzyXAIxK40G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hint\n"
      ],
      "metadata": {
        "id": "PUEDlQytKm6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install -qU langchain langchain-openai langchain-google-genai langchain-together"
      ],
      "metadata": {
        "id": "9BFS010wbJ-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Do neccessary imports and call llm\n",
        "messages = [SystemMessage(content=\"You are an AI expert. Explain concepts in a clear and detailed manner.\")]\n",
        "messages.append(HumanMessage(content=\"What is Generative AI?\"))\n",
        "#Give messages to llm and invoke\n",
        "\n",
        "response = gemini_model.invoke(messages)\n",
        "print(response.content)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vvVqy5SUBm2Y",
        "outputId": "0c380f7a-a852-4bff-ad37-6c2fdb7e0148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI expert, I'd be delighted to explain Generative AI in detail.\n",
            "\n",
            "---\n",
            "\n",
            "## What is Generative AI?\n",
            "\n",
            "**Generative AI** is a category of artificial intelligence models designed to **create new, original content** that resembles the data it was trained on, rather than just classifying, predicting, or analyzing existing data. Unlike traditional discriminative AI, which learns to distinguish between different categories (e.g., \"Is this a cat or a dog?\"), generative AI learns the underlying patterns and structures of its input data to produce novel outputs.\n",
            "\n",
            "Think of it this way:\n",
            "*   **Discriminative AI:** Learns to *recognize* a cat.\n",
            "*   **Generative AI:** Learns to *draw a new cat* that has never existed before, based on its understanding of what makes a cat a cat.\n",
            "\n",
            "This ability to \"generate\" makes it incredibly powerful and versatile across various domains.\n",
            "\n",
            "### How Does Generative AI Work? (The Core Mechanism)\n",
            "\n",
            "The fundamental process involves two key steps:\n",
            "\n",
            "1.  **Learning from Data (Training Phase):**\n",
            "    *   Generative models are trained on vast datasets (e.g., millions of images, billions of text passages, hours of audio).\n",
            "    *   During training, the model doesn't just memorize the data; it learns the **probability distribution** and intricate patterns within the data. It understands how different elements relate to each other, the typical structures, styles, and variations.\n",
            "    *   For example, a text generator learns grammar, syntax, semantics, common phrases, and even stylistic nuances of human language. An image generator learns about shapes, colors, textures, lighting, and object compositions.\n",
            "\n",
            "2.  **Generating New Content (Inference Phase):**\n",
            "    *   Once trained, the model can then be prompted (given an instruction, a partial input, or a random seed) to create something new.\n",
            "    *   It samples from the learned probability distribution to construct novel outputs. It's like a sophisticated artist who has studied countless paintings and can now create a brand new one in a specific style, rather than just copying an existing one.\n",
            "    *   The output will share characteristics with the training data but will not be an exact copy.\n",
            "\n",
            "### Key Characteristics and Capabilities\n",
            "\n",
            "*   **Novelty:** Produces genuinely new, unique content that didn't exist in its training dataset.\n",
            "*   **Coherence and Realism:** The generated outputs are often highly realistic, coherent, and contextually appropriate, making them difficult to distinguish from human-created content.\n",
            "*   **Versatility:** Can generate various data types including text, images, audio, video, code, and even synthetic biological sequences.\n",
            "*   **Conditional Generation:** Can generate content based on specific inputs or prompts (e.g., \"create an image of a cat riding a skateboard,\" \"write a poem about space travel\").\n",
            "*   **Data Augmentation:** Can create synthetic data to expand training datasets for other AI models, especially useful in domains where real data is scarce or sensitive.\n",
            "\n",
            "### Popular Generative AI Architectures\n",
            "\n",
            "Several different model architectures have been developed for generative AI, each with its strengths:\n",
            "\n",
            "1.  **Generative Adversarial Networks (GANs):**\n",
            "    *   **How it works:** Consists of two neural networks, a **Generator** and a **Discriminator**, that compete against each other.\n",
            "        *   The **Generator** tries to create realistic fake data (e.g., images) to fool the Discriminator.\n",
            "        *   The **Discriminator** tries to distinguish between real data (from the training set) and fake data (from the Generator).\n",
            "    *   **Analogy:** A forger (Generator) trying to create perfect fakes, and a detective (Discriminator) trying to spot them. Both improve over time.\n",
            "    *   **Strengths:** Excellent for generating highly realistic images and videos.\n",
            "    *   **Examples:** StyleGAN, BigGAN.\n",
            "\n",
            "2.  **Variational Autoencoders (VAEs):**\n",
            "    *   **How it works:** A type of autoencoder that learns a compressed, continuous representation (called a \"latent space\") of the input data. It encodes data into this latent space and then decodes it back to reconstruct the original. By sampling from this latent space, it can generate new data.\n",
            "    *   **Strengths:** Good for learning the underlying structure of data and enabling controllable generation by manipulating the latent space.\n",
            "    *   **Examples:** Generating faces with specific features (e.g., smiling, wearing glasses).\n",
            "\n",
            "3.  **Transformer-based Models (especially Decoder-only):**\n",
            "    *   **How it works:** These models use an \"attention mechanism\" to weigh the importance of different parts of the input data when generating output. Decoder-only transformers are particularly effective for sequential data. They predict the next word, pixel, or token based on the preceding context.\n",
            "    *   **Strengths:** Revolutionized Natural Language Processing (NLP) and are behind Large Language Models (LLMs). Excellent for text generation, code generation, and increasingly used for images (e.g., DALL-E 2's text encoder is a transformer).\n",
            "    *   **Examples:** GPT-3, GPT-4 (for text), LLaMA, PaLM.\n",
            "\n",
            "4.  **Diffusion Models:**\n",
            "    *   **How it works:** These models learn to reverse a gradual \"noising\" process. During training, noise is progressively added to an image until it becomes pure noise. The model then learns to reverse this process, step by step, to reconstruct the original image from noise. To generate a new image, it starts with pure noise and iteratively \"denoises\" it into a coherent image.\n",
            "    *   **Strengths:** Currently state-of-the-art for high-quality, diverse image generation and are rapidly being adapted for video and audio.\n",
            "    *   **Examples:** DALL-E 2, Stable Diffusion, Midjourney.\n",
            "\n",
            "### Applications of Generative AI\n",
            "\n",
            "The potential applications are vast and continue to expand:\n",
            "\n",
            "*   **Content Creation:**\n",
            "    *   **Text:** Writing articles, stories, poems, marketing copy, scripts, emails, chatbots, code.\n",
            "    *   **Images:** Creating art, realistic photos, product designs, architectural visualizations, digital avatars, synthetic data for training other AI models.\n",
            "    *   **Audio:** Composing music, generating realistic voiceovers, creating sound effects.\n",
            "    *   **Video:** Generating synthetic footage, animation, deepfakes, virtual backgrounds.\n",
            "*   **Design & Engineering:**\n",
            "    *   **Product Design:** Generating novel design concepts for industrial products.\n",
            "    *   **Drug Discovery:** Designing new molecules with desired properties for pharmaceuticals.\n",
            "    *   **Material Science:** Discovering new materials with specific characteristics.\n",
            "*   **Personalization:** Creating highly personalized content, recommendations, and experiences.\n",
            "*   **Data Augmentation:** Generating synthetic data to bolster small or imbalanced datasets for machine learning training.\n",
            "*   **Code Generation:** Writing code snippets, functions, or even entire programs based on natural language descriptions.\n",
            "\n",
            "### Challenges and Ethical Considerations\n",
            "\n",
            "While powerful, Generative AI also presents significant challenges:\n",
            "\n",
            "*   **Bias:** Models can inherit and amplify biases present in their training data, leading to unfair or discriminatory outputs.\n",
            "*   **Misinformation and Deepfakes:** The ability to create highly realistic fake images, audio, and video raises concerns about the spread of misinformation, propaganda, and identity fraud.\n",
            "*   **Copyright and Ownership:** Who owns the content generated by AI? Is it derivative of the training data? This is a complex legal and ethical question.\n",
            "*   **Job Displacement:** Potential impact on creative industries (artists, writers, designers) as AI tools become more capable.\n",
            "*   **Energy Consumption:** Training large generative models requires immense computational resources and energy.\n",
            "*   **Hallucinations:** Generative models, especially LLMs, can sometimes generate plausible-sounding but entirely false information.\n",
            "*   **Security:** Potential for generating malicious code, phishing content, or exploiting vulnerabilities.\n",
            "\n",
            "### Future Outlook\n",
            "\n",
            "Generative AI is one of the most rapidly evolving fields in AI. We can expect:\n",
            "\n",
            "*   **More Sophisticated Models:** Generating even more realistic, complex, and controllable content across modalities.\n",
            "*   **Multimodal Generation:** Models that can seamlessly generate content that combines text, images, audio, and video from a single prompt.\n",
            "*   **Personalized AI:** Highly customized generative agents for individual users.\n",
            "*   **Integration:** Generative capabilities will be integrated into a wide array of tools and applications, from office software to specialized design platforms.\n",
            "*   **Ethical Frameworks:** Increasing focus on developing robust ethical guidelines, regulations, and detection methods for AI-generated content.\n",
            "\n",
            "In conclusion, Generative AI represents a paradigm shift in how we interact with and create digital content. It's moving us from a world of consuming pre-existing information to one where we can actively co-create with intelligent machines, opening up unprecedented possibilities and challenges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghhGBJY4PFEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Exercise 2 : Multi-Turn Conversation\n",
        "\n",
        "Start with a `SystemMessage` defining the LLM's role as a tutor. The user asks a question, and the LLM responds. Then, the user asks a follow-up question, and the LLM provides a further explanation, all while maintaining its role as a tutor.\n",
        "\n"
      ],
      "metadata": {
        "id": "XTZI666-Ah-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hint\n"
      ],
      "metadata": {
        "id": "0S3ioDkAKrki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "# System message defining the LLM's role\n",
        "messages = [SystemMessage(content=\"You are a knowledgeable tutor. Your goal is to explain complex topics in a simple and engaging way.\")]\n",
        "messages.append(HumanMessage(content=\"What is backpropagation in neural networks in 2 sentences?\"))\n",
        "# now ask questions from llm and append few more human and ai messages\n",
        "\n",
        "response1 = gemini_model.invoke(messages)\n",
        "print(\"First response: \", response1.content)\n",
        "\n",
        "#logging follow-up question based on response 1:\n",
        "\n",
        "messages.append(AIMessage(content=response1.content))\n",
        "messages.append(HumanMessage(content=\"Can you explain it more in depth? Up to 5 sentences.\"))\n",
        "\n",
        "response2 = gemini_model.invoke(messages)\n",
        "print(\"Second response: \", response2.content)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aL8laFjJAiGL",
        "outputId": "4a693ea2-a4ee-446f-9a9f-5a6e81cca2c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First response:  Backpropagation is the fundamental algorithm that allows neural networks to learn by efficiently adjusting their internal connections (weights) to make more accurate predictions. It works by calculating how much the network's output error is due to each connection and then propagating this information backward through the layers to update the weights in a way that reduces future errors.\n",
            "Second response:  Backpropagation is the fundamental algorithm that allows neural networks to learn by efficiently adjusting their internal connections. It begins with a \"forward pass\" where input data travels through the network, producing an output prediction. This prediction is then compared to the actual target, and the difference (error) is calculated. The \"backward pass\" then propagates this error backward through the network, layer by layer, to determine how much each individual connection (weight) contributed to the overall error. This information, called the gradient, is then used to update each weight, nudging them in a direction that reduces the error, making the network more accurate for future predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LOfam_5PFpq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}